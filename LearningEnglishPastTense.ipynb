{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quiet-argentina",
   "metadata": {},
   "source": [
    "# Learning English Past Tense: In Response to Rumelhart and McClelland\n",
    "### Michael Ginn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-backup",
   "metadata": {},
   "source": [
    "In 1985, Rumelhart and McClelland helped to pioneer the emerging field of neural network models for language processing in their paper, *On Learning the Past Tenses of English Verbs*. Certainly, this was a critical landmark in the connectionist approach to linguistics, but Rumelhart and McClelland were limited by the technology and theoretical knowledge of their time. \n",
    "\n",
    "Thus, the model they created was somewhat limited in its effectiveness. While it made correct conclusions and errors that matched human language acquisition, it also produced highly unusual errors such as predicting \"membled\" as the past tense form of \"mail\".\n",
    "\n",
    "Nowadays, neural networks are incredibly easy to build and train even with very complex models. Furthermore, our understanding of connectist linguistics has greatly improved. There is now much evidence that morphology has a role in language acquisition, and that considering morphological information can be very beneficial on neural networks (Rueckl & Raveh 1999). \n",
    "\n",
    "Thus, the goal of this notebook is to build a model using modern techniques to solve the same problem as Rumelhart and McClelland, predicting the past tense forms of English verbs. However, we will make a few changes, as described below.\n",
    "\n",
    "### Deep Learning\n",
    "The Rumelhart and McClelland (from here on, R&M) paper uses a single-layer perceptron that maps directly from input to output features, primarily due to technological limits of the time. Modern machine learning theory tends heavily toward deep models, where there are multiple \"hidden\" layers in between the input and output layers. These layers are critical in representing patterns where data is not linearly separable and allow neural networks to achieve very accurate results. Thus, our model will use hidden layers. The number and size of these layers will be decided through cross-validation.\n",
    "\n",
    "### Morphological Features\n",
    "The R&M model maps word inputs to Wickelfeatures, which encodes each letter as the previous and following letter. Through this, the model is trained with each stem as the input and their past tense form as the output.\n",
    "\n",
    "![The R&M network architecture](./model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-brook",
   "metadata": {},
   "source": [
    "It is clear, then, that this model makes no consideration of morphology. If we assume that learners utilize morphological information, then this model clearly misses out on valuable information. It is akin to attempting to classify an image by looking only at the raw pixels, without using the convolutional features that all modern computer vision models use. \n",
    "\n",
    "Therefore, our model will train on morphology. Specifically, our input will be phonological representations of the stem of a verb, and the output will be one of the possible past tense morphemes, such as -ed or -u-. This allows our model to predict the past tense morpheme for a new word, and thereby to predict the full form, assuming that other processes handle the phonological transformation.\n",
    "\n",
    "The question arises, however, of whether this learner learns similar to a human learner. After all, humans are not explicitly provided with morphemes and stems. \n",
    "\n",
    "However, I would argue that the separation of words into morphemes is a separate process, which could also be modeled (but is out of scope for this project). A learner is unlikely to be able to divide a word such as \"ran\" or \"walked\" into constituent morphemes, unless A) they have encountered the word before, B) they have seen similar words and can infer this word's makeup, and/or C) the word is in context and they predict that because it is an inflected form, it has multiple morphemes. Thus, we could construct an algorithm that does this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-glossary",
   "metadata": {},
   "source": [
    "Furthermore, note that this model retains the intuition that a key factor in learner predictions on novel data is phonological similarity. Learners will often predict the *-ed* morpheme for most words, unless they are very similar to another word, such as the imaginary \"zwing\", which some learners predict to inflect to \"zwang\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-chair",
   "metadata": {},
   "source": [
    "### Other Models\n",
    "Last, neural networks are not the only structure used in modern data science, and we know that human brains do not necessarily work like neural networks. Thus, it is possible that other models could be very effective in learning past tense inflection, and thus might provide insight into how human learners learn. Specifically, we will look at a nearest neighbors classification model, which uses similarity between data points to predict their class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-concentration",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This project will take the following steps.\n",
    "1. First, we will prepare data by producing a representation of stems and past tense morphemes.\n",
    "2. Next, we will cross-validate to determine optimal hyperparameters for the model.\n",
    "3. We will train the model on a subset of the data.\n",
    "4. We will evaluate the model on the unseen data, comparing our results to the R&M paper. We will also discuss the learning process of the model and the sorts of errors it makes.\n",
    "5. Last, we will repeat steps 2-4 with a k-Nearest Neighbors model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-universal",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "First, we will import some data on English verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "serious-shower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 1747),\n",
       " ('produced', 28),\n",
       " ('took', 426),\n",
       " ('recommended', 17),\n",
       " ('commented', 16),\n",
       " ('urged', 21),\n",
       " ('found', 268),\n",
       " ('added', 81),\n",
       " ('praised', 8),\n",
       " ('filed', 12),\n",
       " ('charged', 17),\n",
       " ('listed', 11),\n",
       " ('became', 246),\n",
       " ('announced', 53),\n",
       " ('brought', 133),\n",
       " ('attended', 24),\n",
       " ('wanted', 204),\n",
       " ('voted', 22),\n",
       " ('resigned', 7),\n",
       " ('approved', 12),\n",
       " ('opened', 94),\n",
       " ('told', 286),\n",
       " ('passed', 91),\n",
       " ('pointed', 48),\n",
       " ('rejected', 12),\n",
       " ('asked', 300),\n",
       " ('tossed', 22),\n",
       " ('put', 130),\n",
       " ('saw', 338),\n",
       " ('defeated', 5)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "\n",
    "# Find all the instances from the corpus of verbs that are past tense\n",
    "past_verbs = {} # Will hold words and their frequency counts\n",
    "for w in list(filter(lambda w: w[1] == 'VBD', brown.tagged_words())):\n",
    "    word = w[0].lower()\n",
    "    past_verbs[word] = 1 if not word in past_verbs else past_verbs[word] + 1\n",
    "        \n",
    "past_verbs = list(past_verbs.items())\n",
    "\n",
    "past_verbs[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educated-retreat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/milesper/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we'll lemmatize these words into their bare forms.\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "departmental-eagle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'take'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize(\"took\", 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-while",
   "metadata": {},
   "source": [
    "Notice that this lemmatizer fails in a few cases where words may be homophones, such as \"saw\". We will have to correct these manually. We can detect this by looking at the instances where the lemmatized form is identical to the input form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infinite-response",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 1747, 'say'),\n",
       " ('produced', 28, 'produce'),\n",
       " ('took', 426, 'take'),\n",
       " ('recommended', 17, 'recommend'),\n",
       " ('commented', 16, 'comment'),\n",
       " ('urged', 21, 'urge'),\n",
       " ('found', 268, 'find'),\n",
       " ('added', 81, 'add'),\n",
       " ('praised', 8, 'praise'),\n",
       " ('filed', 12, 'file'),\n",
       " ('charged', 17, 'charge'),\n",
       " ('listed', 11, 'list'),\n",
       " ('became', 246, 'become'),\n",
       " ('announced', 53, 'announce'),\n",
       " ('brought', 133, 'bring'),\n",
       " ('attended', 24, 'attend'),\n",
       " ('wanted', 204, 'want'),\n",
       " ('voted', 22, 'vote'),\n",
       " ('resigned', 7, 'resign'),\n",
       " ('approved', 12, 'approve'),\n",
       " ('opened', 94, 'open'),\n",
       " ('told', 286, 'tell'),\n",
       " ('passed', 91, 'pass'),\n",
       " ('pointed', 48, 'point'),\n",
       " ('rejected', 12, 'reject'),\n",
       " ('asked', 300, 'ask'),\n",
       " ('tossed', 22, 'toss'),\n",
       " ('put', 130, 'put'),\n",
       " ('saw', 338, 'saw'),\n",
       " ('defeated', 5, 'defeat')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_forms = [(v[0], v[1], lemmatizer.lemmatize(v[0], 'v')) for v in past_verbs]\n",
    "lemmatized_forms[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-mauritius",
   "metadata": {},
   "source": [
    "Let's find all the instances where the lemmatized form is identical. This will let us correct the mis-lemmatizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "textile-fiber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 ('put', 130, 'put')\n",
      "28 ('saw', 338, 'saw')\n",
      "64 ('felt', 302, 'felt')\n",
      "153 ('read', 36, 'read')\n",
      "209 ('beat', 12, 'beat')\n",
      "216 ('cut', 25, 'cut')\n",
      "224 ('set', 71, 'set')\n",
      "235 ('bet', 1, 'bet')\n",
      "260 ('lay', 81, 'lay')\n",
      "267 ('hit', 38, 'hit')\n",
      "275 ('gruonded', 1, 'gruonded')\n",
      "314 ('cost', 10, 'cost')\n",
      "316 ('fell', 87, 'fell')\n",
      "353 (\"bulletin'd\", 1, \"bulletin'd\")\n",
      "475 ('bid', 1, 'bid')\n",
      "567 ('double-bogeyed', 1, 'double-bogeyed')\n",
      "709 ('let', 37, 'let')\n",
      "717 ('burst', 11, 'burst')\n",
      "733 ('upset', 1, 'upset')\n",
      "770 ('thrust', 9, 'thrust')\n",
      "802 ('spread', 18, 'spread')\n",
      "838 ('cast', 4, 'cast')\n",
      "863 ('bore', 14, 'bore')\n",
      "873 ('shed', 3, 'shed')\n",
      "907 ('underlay', 1, 'underlay')\n",
      "931 ('spit', 3, 'spit')\n",
      "961 ('hop-skipped', 1, 'hop-skipped')\n",
      "1073 ('bed-hopped', 1, 'bed-hopped')\n",
      "1093 ('co-operated', 1, 'co-operated')\n",
      "1120 ('sowered', 1, 'sowered')\n",
      "1122 ('thout', 1, 'thout')\n",
      "1123 ('recond', 1, 'recond')\n",
      "1124 ('sed', 1, 'sed')\n",
      "1125 ('though', 1, 'though')\n",
      "1127 ('brok', 1, 'brok')\n",
      "1128 ('run', 1, 'run')\n",
      "1129 ('holored', 1, 'holored')\n",
      "1130 ('recooned', 1, 'recooned')\n",
      "1251 ('beset', 1, 'beset')\n",
      "1338 ('rid', 1, 'rid')\n",
      "1370 ('incanted', 1, 'incanted')\n",
      "1465 ('sin-ned', 1, 'sin-ned')\n",
      "1481 ('overlay', 1, 'overlay')\n",
      "1496 ('affied', 1, 'affied')\n",
      "1499 ('tooke', 2, 'tooke')\n",
      "1503 ('druncke', 1, 'druncke')\n",
      "1504 ('drewe', 1, 'drewe')\n",
      "1505 ('commawnded', 1, 'commawnded')\n",
      "1506 ('punnished', 1, 'punnished')\n",
      "1517 ('incepted', 1, 'incepted')\n",
      "1535 ('quit', 2, 'quit')\n",
      "1553 ('gutted', 1, 'gutted')\n",
      "1581 ('re-declared', 1, 're-declared')\n",
      "1611 ('reemerged', 1, 'reemerged')\n",
      "1630 ('re-used', 1, 're-used')\n",
      "1694 ('split', 5, 'split')\n",
      "1740 ('et', 1, 'et')\n",
      "1756 ('wet', 2, 'wet')\n",
      "1795 ('smelt', 3, 'smelt')\n",
      "1822 ('spat', 7, 'spat')\n",
      "1824 ('shut', 7, 'shut')\n",
      "1844 ('lucked', 1, 'lucked')\n",
      "1900 ('doled', 1, 'doled')\n",
      "1928 ('double-crossed', 1, 'double-crossed')\n",
      "1964 ('grokked', 3, 'grokked')\n",
      "2020 ('side-stepped', 1, 'side-stepped')\n",
      "2024 ('broadcast', 1, 'broadcast')\n",
      "2036 ('half-reached', 1, 'half-reached')\n",
      "2038 ('half-straightened', 1, 'half-straightened')\n",
      "2064 ('reentered', 1, 'reentered')\n",
      "2069 ('holstered', 1, 'holstered')\n",
      "2093 ('tootley-toot-tootled', 1, 'tootley-toot-tootled')\n",
      "2095 ('tole', 1, 'tole')\n",
      "2153 ('loused', 1, 'loused')\n",
      "2184 ('figgered', 1, 'figgered')\n",
      "2201 ('pfffted', 1, 'pfffted')\n",
      "2202 ('straight-armed', 1, 'straight-armed')\n",
      "2208 ('self-served', 1, 'self-served')\n",
      "2210 ('fisted', 1, 'fisted')\n",
      "2263 ('hurt', 1, 'hurt')\n",
      "2271 ('refolded', 1, 'refolded')\n",
      "2277 ('fit', 1, 'fit')\n",
      "2305 ('meted', 1, 'meted')\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(lemmatized_forms):\n",
    "    if not w[0] == w[2]: continue\n",
    "    print(i, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-heather",
   "metadata": {},
   "source": [
    "Now let's fix these words if necessary. We will also remove some weird instances such as \"pfffted\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "similar-street",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 ('put', 130, 'put')\n",
      "153 ('read', 36, 'read')\n",
      "209 ('beat', 12, 'beat')\n",
      "216 ('cut', 25, 'cut')\n",
      "224 ('set', 71, 'set')\n",
      "235 ('bet', 1, 'bet')\n",
      "260 ('lay', 81, 'lay')\n",
      "267 ('hit', 38, 'hit')\n",
      "313 ('cost', 10, 'cost')\n",
      "473 ('bid', 1, 'bid')\n",
      "706 ('let', 37, 'let')\n",
      "714 ('burst', 11, 'burst')\n",
      "730 ('upset', 1, 'upset')\n",
      "767 ('thrust', 9, 'thrust')\n",
      "799 ('spread', 18, 'spread')\n",
      "835 ('cast', 4, 'cast')\n",
      "860 ('bore', 14, 'bore')\n",
      "870 ('shed', 3, 'shed')\n",
      "927 ('spit', 3, 'spit')\n",
      "1235 ('beset', 1, 'beset')\n",
      "1322 ('rid', 1, 'rid')\n",
      "1510 ('quit', 2, 'quit')\n",
      "1666 ('split', 5, 'split')\n",
      "1727 ('wet', 2, 'wet')\n",
      "1794 ('shut', 7, 'shut')\n",
      "1991 ('broadcast', 1, 'broadcast')\n",
      "2221 ('hurt', 1, 'hurt')\n",
      "2235 ('fit', 1, 'fit')\n"
     ]
    }
   ],
   "source": [
    "corrected_lemmatized_forms = lemmatized_forms\n",
    "corrected_lemmatized_forms[28] = ('saw', 337, 'see')\n",
    "corrected_lemmatized_forms[64] = ('felt', 302, 'feel')\n",
    "corrected_lemmatized_forms[316] = ('fell', 87, 'fall')\n",
    "corrected_lemmatized_forms[1370] = ('incanted', 1, 'incant')\n",
    "corrected_lemmatized_forms[1611] = ('reemerged', 1, 'reemerge')\n",
    "corrected_lemmatized_forms[1795] = ('smelt', 3, 'smelted')\n",
    "corrected_lemmatized_forms[1844] = ('lucked', 1, 'luck')\n",
    "corrected_lemmatized_forms[1900] = ('doled', 1, 'dole')\n",
    "corrected_lemmatized_forms[2064] = ('reentered', 1, 'reenter')\n",
    "corrected_lemmatized_forms[2069] = ('holstered', 1, 'holster')\n",
    "corrected_lemmatized_forms[2210] = ('fisted', 1, 'fist')\n",
    "corrected_lemmatized_forms[2271] = ('refolded', 1, 'refold')\n",
    "\n",
    "indices_to_delete = [275, 353, 567, 907, 961, 1073, 1093, 1120, 1122, 1123, \n",
    "                     1124, 1125, 1127, 1128, 1129, 1130, 1465, 1481, 1496, 1499,\n",
    "                     1503, 1504, 1505, 1506, 1517, 1553, 1581, 1630, 1740, 1822, \n",
    "                     1928, 1964, 2020, 2036, 2038, 2093, 2095, 2153, 2184, 2201, \n",
    "                     2202, 2208, 2305\n",
    "                    ]\n",
    "\n",
    "for index in reversed(indices_to_delete):\n",
    "    del corrected_lemmatized_forms[index]\n",
    "    \n",
    "for i, w in enumerate(corrected_lemmatized_forms):\n",
    "    if not w[0] == w[2]: continue\n",
    "    print(i, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-concept",
   "metadata": {},
   "source": [
    "### Creating Phonetic Representations\n",
    "Now, let's convert both forms into phonetic representations. There is one problem: some orthographic representations have multiple pronounciations, which are critical in past tense formation. For instance, we must distinguish between \"read\" (present) and \"read\" (past). We will account for this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ancient-washer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cmudict in /Users/milesper/Library/Python/3.7/lib/python/site-packages (1.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['W', 'AA1', 'N', 'T'], ['W', 'AO1', 'N', 'T']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install cmudict\n",
    "import cmudict\n",
    "\n",
    "cmu = cmudict.dict()\n",
    "\n",
    "def pronounce(word):\n",
    "    if word in cmu:\n",
    "        return cmudict.dict().get(word)\n",
    "    else:\n",
    "        return [None]\n",
    "\n",
    "pronounce(\"want\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-tradition",
   "metadata": {},
   "source": [
    "This next step is very slow. Therefore, in order to retrieve the stored value, we use the `%store` command. If you don't want to do the calculation, skip the next cell and run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "thousand-invitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b228bcf108494d79803949dada39a73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stored 'phonetic_forms' (list)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "phonetic_forms = []\n",
    "\n",
    "for (verb_past, freq, stem) in tqdm(lemmatized_forms):\n",
    "    past_form_prounciation = pronounce(verb_past)\n",
    "    stem_form_prounciation = pronounce(stem)\n",
    "    phonetic_forms.append((stem_form_prounciation, past_form_prounciation, freq))\n",
    "\n",
    "phonetic_forms[153] = ([['R', 'IY1', 'D']], [['R', 'EH1', 'D']], 36)\n",
    "%store phonetic_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exterior-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r phonetic_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "contrary-russell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([['S', 'EY1']], [['S', 'EH1', 'D']], 1747),\n",
       " ([['P', 'R', 'AH0', 'D', 'UW1', 'S'], ['P', 'R', 'OW1', 'D', 'UW0', 'S']],\n",
       "  [['P', 'R', 'AH0', 'D', 'UW1', 'S', 'T']],\n",
       "  28),\n",
       " ([['T', 'EY1', 'K']], [['T', 'UH1', 'K']], 426),\n",
       " ([['R', 'EH2', 'K', 'AH0', 'M', 'EH1', 'N', 'D']],\n",
       "  [['R', 'EH2', 'K', 'AH0', 'M', 'EH1', 'N', 'D', 'IH0', 'D']],\n",
       "  17),\n",
       " ([['K', 'AA1', 'M', 'EH0', 'N', 'T']],\n",
       "  [['K', 'AA1', 'M', 'EH0', 'N', 'T', 'AH0', 'D']],\n",
       "  16),\n",
       " ([['ER1', 'JH']], [['ER1', 'JH', 'D']], 21),\n",
       " ([['F', 'AY1', 'N', 'D']], [['F', 'AW1', 'N', 'D']], 268),\n",
       " ([['AE1', 'D']], [['AE1', 'D', 'AH0', 'D'], ['AE1', 'D', 'IH0', 'D']], 81),\n",
       " ([['P', 'R', 'EY1', 'Z']], [['P', 'R', 'EY1', 'Z', 'D']], 8),\n",
       " ([['F', 'AY1', 'L']], [['F', 'AY1', 'L', 'D']], 12),\n",
       " ([['CH', 'AA1', 'R', 'JH']], [['CH', 'AA1', 'R', 'JH', 'D']], 17),\n",
       " ([['L', 'IH1', 'S', 'T']],\n",
       "  [['L', 'IH1', 'S', 'T', 'AH0', 'D'], ['L', 'IH1', 'S', 'T', 'IH0', 'D']],\n",
       "  11),\n",
       " ([['B', 'IH0', 'K', 'AH1', 'M']],\n",
       "  [['B', 'IH0', 'K', 'EY1', 'M'], ['B', 'IY0', 'K', 'EY1', 'M']],\n",
       "  246),\n",
       " ([['AH0', 'N', 'AW1', 'N', 'S']], [['AH0', 'N', 'AW1', 'N', 'S', 'T']], 53),\n",
       " ([['B', 'R', 'IH1', 'NG']], [['B', 'R', 'AO1', 'T']], 133),\n",
       " ([['AH0', 'T', 'EH1', 'N', 'D']],\n",
       "  [['AH0', 'T', 'EH1', 'N', 'D', 'IH0', 'D']],\n",
       "  24),\n",
       " ([['W', 'AA1', 'N', 'T'], ['W', 'AO1', 'N', 'T']],\n",
       "  [['W', 'AO1', 'N', 'T', 'IH0', 'D']],\n",
       "  204),\n",
       " ([['V', 'OW1', 'T']], [['V', 'OW1', 'T', 'IH0', 'D']], 22),\n",
       " ([['R', 'IH0', 'Z', 'AY1', 'N'],\n",
       "   ['R', 'IY0', 'Z', 'AY1', 'N'],\n",
       "   ['R', 'IY0', 'S', 'AY1', 'N']],\n",
       "  [['R', 'IH0', 'Z', 'AY1', 'N', 'D'],\n",
       "   ['R', 'IY0', 'Z', 'AY1', 'N', 'D'],\n",
       "   ['R', 'IY0', 'S', 'AY1', 'N', 'D']],\n",
       "  7),\n",
       " ([['AH0', 'P', 'R', 'UW1', 'V']], [['AH0', 'P', 'R', 'UW1', 'V', 'D']], 12)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_forms[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "guilty-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{ stem: 'say', past: 'said' },\n",
       " { stem: 'produce', past: 'produced' },\n",
       " { stem: 'take', past: 'took' },\n",
       " { stem: 'recommend', past: 'recommended' },\n",
       " { stem: 'comment', past: 'commented' },\n",
       " { stem: 'urge', past: 'urged' },\n",
       " { stem: 'find', past: 'found' },\n",
       " { stem: 'add', past: 'added' },\n",
       " { stem: 'praise', past: 'praised' },\n",
       " { stem: 'file', past: 'filed' },\n",
       " { stem: 'charge', past: 'charged' },\n",
       " { stem: 'list', past: 'listed' },\n",
       " { stem: 'become', past: 'became' },\n",
       " { stem: 'announce', past: 'announced' },\n",
       " { stem: 'bring', past: 'brought' },\n",
       " { stem: 'attend', past: 'attended' },\n",
       " { stem: 'want', past: 'wanted' },\n",
       " { stem: 'vote', past: 'voted' },\n",
       " { stem: 'resign', past: 'resigned' },\n",
       " { stem: 'approve', past: 'approved' },\n",
       " { stem: 'open', past: 'opened' },\n",
       " { stem: 'tell', past: 'told' },\n",
       " { stem: 'pass', past: 'passed' },\n",
       " { stem: 'point', past: 'pointed' },\n",
       " { stem: 'reject', past: 'rejected' },\n",
       " { stem: 'ask', past: 'asked' },\n",
       " { stem: 'toss', past: 'tossed' },\n",
       " { stem: 'put', past: 'put' },\n",
       " { stem: 'see', past: 'saw' },\n",
       " { stem: 'defeat', past: 'defeated' },\n",
       " { stem: 'receive', past: 'received' },\n",
       " { stem: 'get', past: 'got' },\n",
       " { stem: 'stand', past: 'stood' },\n",
       " { stem: 'shoot', past: 'shot' },\n",
       " { stem: 'schedule', past: 'scheduled' },\n",
       " { stem: 'fear', past: 'feared' },\n",
       " { stem: 'promise', past: 'promised' },\n",
       " { stem: 'make', past: 'made' },\n",
       " { stem: 'go', past: 'went' },\n",
       " { stem: 'seem', past: 'seemed' },\n",
       " { stem: 'lead', past: 'led' },\n",
       " { stem: 'taunt', past: 'taunted' },\n",
       " { stem: 'leave', past: 'left' },\n",
       " { stem: 'term', past: 'termed' },\n",
       " { stem: 'draft', past: 'drafted' },\n",
       " { stem: 'declare', past: 'declared' },\n",
       " { stem: 'sound', past: 'sounded' },\n",
       " { stem: 'argue', past: 'argued' },\n",
       " { stem: 'complain', past: 'complained' },\n",
       " { stem: 'estimate', past: 'estimated' },\n",
       " { stem: 'hear', past: 'heard' },\n",
       " { stem: 'send', past: 'sent' },\n",
       " { stem: 'whip', past: 'whipped' },\n",
       " { stem: 'validate', past: 'validated' },\n",
       " { stem: 'enlarge', past: 'enlarged' },\n",
       " { stem: 'amend', past: 'amended' },\n",
       " { stem: 'suggest', past: 'suggested' },\n",
       " { stem: 'decide', past: 'decided' },\n",
       " { stem: 'keep', past: 'kept' },\n",
       " { stem: 'report', past: 'reported' },\n",
       " { stem: 'lose', past: 'lost' },\n",
       " { stem: 'insist', past: 'insisted' },\n",
       " { stem: 'end', past: 'ended' },\n",
       " { stem: 'construe', past: 'construed' },\n",
       " { stem: 'feel', past: 'felt' },\n",
       " { stem: 'propose', past: 'proposed' },\n",
       " { stem: 'join', past: 'joined' },\n",
       " { stem: 'call', past: 'called' },\n",
       " { stem: 'startle', past: 'startled' },\n",
       " { stem: 'indicate', past: 'indicated' },\n",
       " { stem: 'know', past: 'knew' },\n",
       " { stem: 'lean', past: 'leaned' },\n",
       " { stem: 'inquire', past: 'inquired' },\n",
       " { stem: 'reply', past: 'replied' },\n",
       " { stem: 'push', past: 'pushed' },\n",
       " { stem: 'spend', past: 'spent' },\n",
       " { stem: 'talk', past: 'talked' },\n",
       " { stem: 'return', past: 'returned' },\n",
       " { stem: 'involve', past: 'involved' },\n",
       " { stem: 'obtain', past: 'obtained' },\n",
       " { stem: 'pertain', past: 'pertained' },\n",
       " { stem: 'rule', past: 'ruled' },\n",
       " { stem: 'admit', past: 'admitted' },\n",
       " { stem: 'issue', past: 'issued' },\n",
       " { stem: 'question', past: 'questioned' },\n",
       " { stem: 'constitute', past: 'constituted' },\n",
       " { stem: 'tie', past: 'tied' },\n",
       " { stem: 'sponsor', past: 'sponsored' },\n",
       " { stem: 'note', past: 'noted' },\n",
       " { stem: 'oppose', past: 'opposed' },\n",
       " { stem: 'criticize', past: 'criticized' },\n",
       " { stem: 'back', past: 'backed' },\n",
       " { stem: 'hail', past: 'hailed' },\n",
       " { stem: 'confirm', past: 'confirmed' },\n",
       " { stem: 'describe', past: 'described' },\n",
       " { stem: 'hasten', past: 'hastened' },\n",
       " { stem: 'come', past: 'came' },\n",
       " { stem: 'give', past: 'gave' },\n",
       " { stem: 'agree', past: 'agreed' },\n",
       " { stem: 'serve', past: 'served' }]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "master_corpus = []\n",
    "\n",
    "class CorpusItem:\n",
    "    def __init__(self, stem, past_form, stem_phonetic, past_form_phonetic, frequency):\n",
    "        self.stem = stem\n",
    "        self.past_form = past_form\n",
    "        self.stem_phonetic = stem_phonetic\n",
    "        self.past_form_phonetic = past_form_phonetic\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"{ stem: '\" + self.stem + \"', past: '\" + self.past_form + \"' }\"\n",
    "\n",
    "for ((verb_past, freq, stem), (stem_form_pronounciation, past_form_pronounciation, _)) in zip(lemmatized_forms, phonetic_forms):\n",
    "    if stem_form_pronounciation == [None] or past_form_pronounciation == [None]:\n",
    "        continue\n",
    "    \n",
    "    # Strip numbers\n",
    "    stem_form_pronounciation = [[re.sub(r'\\d+', '', alt) for alt in p] for p in stem_form_pronounciation]\n",
    "    past_form_pronounciation = [[re.sub(r'\\d+', '', alt) for alt in p] for p in past_form_pronounciation]\n",
    "    \n",
    "    master_corpus.append(CorpusItem(stem, verb_past, stem_form_pronounciation, past_form_pronounciation, freq))\n",
    "    \n",
    "master_corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "delayed-deficit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['S', 'AO']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_corpus_item(stem, past=None, corpus=master_corpus):\n",
    "    \"\"\"Finds a specified item in the corpus\"\"\"\n",
    "    for item in corpus:\n",
    "        if item.stem == stem:\n",
    "            if not (past and not item.past_form == past):\n",
    "                return item\n",
    "    return \"Item does not exist!!\"\n",
    "\n",
    "find_corpus_item(\"see\").past_form_phonetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-ontario",
   "metadata": {},
   "source": [
    "### Determining Past Tense Morphemes\n",
    "The next step is to determine which past tense morpheme is used in each word. We will consider only the underlying form of the morphemes, so both \"passed\", \"pointed\", and \"attended\" all use the `/-ed/` morpheme. We will do this by diffing the stem and past tense form and determining which morpheme the diff is most likely to indicate.\n",
    "\n",
    "Earlier, we stored all possible pronounciations for the stem and past tense form. Now, we will try diffing every possible combination, and using whichever two forms produce the shortest combination. This allows us to get two forms that are as similar as possible, so that the true morpheme is produced. For instance, \"estimate\" can be prounounced differently depending on whether it is a noun or verb. However, the verb form should sound more similar to \"estimated\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "color-police",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['- W', '- ER', '- K', '+ R', '+ AO', '+ T'],\n",
       " ['+ r', '- r', '- k', '+ u', '+ g', '+ h', '+ t'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def diff(corpus_item: CorpusItem):\n",
    "    \"\"\"Produces a list of the changes between the stem and past tense form\"\"\"\n",
    "    \n",
    "    # When we run this function on an item, we replace the list of prounounciations with the best ones. Thus, we need to check if this already happened on this item.\n",
    "    if isinstance(corpus_item.stem_phonetic[0], list):\n",
    "        best_stem = corpus_item.stem_phonetic[0]\n",
    "        best_past = corpus_item.past_form_phonetic[0]\n",
    "        shortest_diff = list(filter(lambda d: d[0] != ' ', difflib.ndiff(best_stem, best_past)))\n",
    "        for stem_phonetic in corpus_item.stem_phonetic:\n",
    "            for past_form_phonetic in corpus_item.past_form_phonetic:\n",
    "                diff = list(filter(lambda d: d[0] != ' ', difflib.ndiff(stem_phonetic, past_form_phonetic)))\n",
    "                if len(diff) < len(shortest_diff):\n",
    "                    best_stem = stem_phonetic\n",
    "                    best_past = past_form_phonetic\n",
    "                    shortest_diff = diff\n",
    "                \n",
    "        corpus_item.stem_phonetic = best_stem\n",
    "        corpus_item.past_form_phonetic = best_past\n",
    "    else:\n",
    "        shortest_diff = list(filter(lambda d: d[0] != ' ', difflib.ndiff(corpus_item.stem_phonetic, corpus_item.past_form_phonetic)))\n",
    "    \n",
    "    # In some cases, the orthographic form will help us determine the morpheme, so let's diff that as well\n",
    "    ortho_diff = list(difflib.ndiff(corpus_item.stem, corpus_item.past_form))\n",
    "    ortho_diff = list(filter(lambda d: d[0] != ' ', ortho_diff))\n",
    "    return (shortest_diff, ortho_diff)\n",
    "\n",
    "diff(find_corpus_item(\"work\", \"wrought\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-glasgow",
   "metadata": {},
   "source": [
    "Rumelhart and McClelland list nine irregular classes in addition to the common `/ed/` morpheme. We will describe these classes as follows.\n",
    "\n",
    "- Class I is the vacuous morpheme.\n",
    "- Class II is a change from /d/ to /t/\n",
    "- Class III and IV represent a vowel change and adding /t/ to the end. I chose to combine these because it seems likely that the distinction between the two is a phonological process, as we might imagine \"bring\" -> \"brɔng\" -> \"brɔngt\" -> \"brɔt\"\n",
    "- Class V, VIa, VIb, VII, and VIII all represent internal vowel changes. We will differentiate only based on which vowel is added, assuming it is an infix. This is a useful distinction because it makes our predictions much more powerful.\n",
    "\n",
    "There is no need to distinguish between allophones/allomorphs of these morphemes, since we are presupposing that morphology and phonology are separate processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "detailed-eating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MORPHEME.VOW_AND_T: 'vow change, add /t/'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "class MORPHEME(str, Enum):\n",
    "    ED        = \"+ed\",   # pass -> pass+ed\n",
    "    NULL      = \"Ø\",     # hit -> hit\n",
    "    D_T       = \"d->t\",   # send -> sent\n",
    "    VOW_AND_T = \"vow change, add /t/\",  # feel -> felt, say -> said\n",
    "    I         = \"+ɪ+\",    # bite -> bit\n",
    "    AU        = \"+aʊ+\",   # find -> found\n",
    "    O         = \"+oʊ+\",   # ride -> rode, break -> broke\n",
    "    AE        = \"+æ+\",    # sing -> sang, drink -> drank\n",
    "    EI        = \"+eɪ+\",   # give -> gave, eat -> ate\n",
    "    UH        = \"+ʌ+\",    # sting -> stung, hang -> hung\n",
    "    U         = \"+ʊ+\",    # take -> took\n",
    "    OO        = \"+u+\",    # fly -> flew, blow -> blew\n",
    "    AH        = \"+ɑ+\",    # shoot -> shot\n",
    "    EH        = \"+ɛ+\",    # speed -> sped\n",
    "    SUPP      = \"suppletion\" # go -> went\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str.__str__(self)\n",
    "\n",
    "    \n",
    "def determine_morpheme(corpus_item: CorpusItem):\n",
    "    \"\"\"Returns the past tense morpheme for a given word\"\"\"\n",
    "    (item_diff, ortho_diff) = diff(corpus_item)\n",
    "    \n",
    "    if len(item_diff) == 0:\n",
    "        return MORPHEME.NULL\n",
    "    \n",
    "    # Catches some weird cases where we have different pronounciations of words\n",
    "    if ortho_diff == ['+ d']:\n",
    "        return MORPHEME.ED\n",
    "    \n",
    "    if len(item_diff) == 1 and (item_diff[0] == \"+ T\" or item_diff[0] == \"+ D\"):\n",
    "        return MORPHEME.ED\n",
    "    if item_diff == ['- D', '+ T']:\n",
    "        return MORPHEME.D_T\n",
    "    if (len(item_diff) == 3 or len(item_diff) == 4) and (item_diff[-1] == \"+ T\" or item_diff[-1] == \"+ D\"):\n",
    "        return MORPHEME.VOW_AND_T\n",
    "    if len(item_diff) == 2:\n",
    "        if \"+ \" in item_diff[0] and (item_diff[1] == \"+ T\" or item_diff[1] == \"+ D\"):\n",
    "            return MORPHEME.ED\n",
    "        if \"+ IH\" in item_diff[1]:\n",
    "            return MORPHEME.I\n",
    "        if \"+ AW\" in item_diff[1]:\n",
    "            return MORPHEME.AU\n",
    "        if \"+ OW\" in item_diff[1]:\n",
    "            return MORPHEME.O\n",
    "        if \"+ AE\" in item_diff[1]:\n",
    "            return MORPHEME.AE\n",
    "        if \"+ EY\" in item_diff[1]:\n",
    "            return MORPHEME.EI\n",
    "        if \"+ AH\" in item_diff[1]:\n",
    "            return MORPHEME.UH\n",
    "        if \"+ UH\" in item_diff[1]:\n",
    "            return MORPHEME.U\n",
    "        if \"+ UW\" in item_diff[1]:\n",
    "            return MORPHEME.OO\n",
    "        if \"+ AA\" in item_diff[1] or \"+ AO\" in item_diff[1]:\n",
    "            return MORPHEME.AH\n",
    "        if \"+ EH\" in item_diff[1]:\n",
    "            return MORPHEME.EH\n",
    "    print(\"Unrecognized morpheme! Assuming suppletion for \" + corpus_item.stem + \" -> \" + corpus_item.past_form)\n",
    "    return MORPHEME.SUPP\n",
    "\n",
    "determine_morpheme(find_corpus_item(\"say\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pacific-fleece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized morpheme! Assuming suppletion for stand -> stood\n",
      "Unrecognized morpheme! Assuming suppletion for make -> made\n",
      "Unrecognized morpheme! Assuming suppletion for go -> went\n",
      "Unrecognized morpheme! Assuming suppletion for leave -> left\n",
      "Unrecognized morpheme! Assuming suppletion for lose -> lost\n",
      "Unrecognized morpheme! Assuming suppletion for think -> thought\n",
      "Unrecognized morpheme! Assuming suppletion for stag -> staged\n",
      "Unrecognized morpheme! Assuming suppletion for rag -> raged\n",
      "Unrecognized morpheme! Assuming suppletion for wag -> waged\n",
      "Unrecognized morpheme! Assuming suppletion for behold -> beheld\n",
      "Unrecognized morpheme! Assuming suppletion for undergo -> underwent\n",
      "Unrecognized morpheme! Assuming suppletion for understand -> understood\n",
      "Unrecognized morpheme! Assuming suppletion for misunderstand -> misunderstood\n",
      "Unrecognized morpheme! Assuming suppletion for bath -> bathed\n",
      "Unrecognized morpheme! Assuming suppletion for withstand -> withstood\n",
      "Unrecognized morpheme! Assuming suppletion for work -> wrought\n"
     ]
    }
   ],
   "source": [
    "for item in master_corpus:\n",
    "    item.morpheme = determine_morpheme(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "jewish-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up some of these.\n",
    "corpus = master_corpus.copy()\n",
    "corpus.remove(find_corpus_item(\"stag\", corpus=corpus))\n",
    "corpus.remove(find_corpus_item(\"rag\", corpus=corpus))\n",
    "corpus.remove(find_corpus_item(\"wag\", corpus=corpus))\n",
    "corpus.remove(find_corpus_item(\"bath\", corpus=corpus))\n",
    "corpus.remove(find_corpus_item(\"work\", \"wrought\", corpus=corpus))\n",
    "find_corpus_item(\"leave\", corpus=corpus).morpheme = MORPHEME.VOW_AND_T\n",
    "find_corpus_item(\"lose\", corpus=corpus).morpheme = MORPHEME.VOW_AND_T\n",
    "find_corpus_item(\"think\", corpus=corpus).morpheme = MORPHEME.VOW_AND_T\n",
    "find_corpus_item(\"behold\", corpus=corpus).morpheme = MORPHEME.EH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "occupational-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining instances of suppletion\n",
      "stand -> stood\n",
      "make -> made\n",
      "go -> went\n",
      "undergo -> underwent\n",
      "understand -> understood\n",
      "misunderstand -> misunderstood\n",
      "withstand -> withstood\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining instances of suppletion\")\n",
    "for item in corpus:\n",
    "    if item.morpheme == MORPHEME.SUPP:\n",
    "        print(item.stem + \" -> \" + item.past_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-second",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "We will build our model using Wickelfeatures as the encoding for the input like the R&M paper. Thus, we will need to encode the stem of every word into Wickelfeatures.\n",
    "\n",
    "![The features](./features.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sharp-guatemala",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#', 'G', 'OW'], ['G', 'OW', '#']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wickelphones(word):\n",
    "    word = [\"#\"] + word + [\"#\"]\n",
    "    phones = []\n",
    "    for index in range(1,len(word)-1):\n",
    "        phones.append(word[index-1:index+2])\n",
    "    return phones\n",
    "\n",
    "wickelphones(find_corpus_item(\"go\", corpus=corpus).stem_phonetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "flying-momentum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vowel', 'type1', 'middle', 'voiced')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As defined by R&M\n",
    "class FEATURES:\n",
    "    INTERRUPTED = [\"B\", \"P\", \"D\", \"T\", \"G\", \"K\", \"M\", \"N\", \"NG\"]\n",
    "    CONT_CONSONANT = [\"V\", \"DH\", \"F\", \"TH\", \"Z\", \"S\", \"ZH\", \"JH\", \"SH\", \"CH\", \"W\", \"L\", \"R\", \"Y\", \"H\", \"HH\"]\n",
    "    VOWEL = [\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AX\", \"AY\", \"AY\", \"EH\", \"ER\", \"EY\", \"IH\", \"IX\", \"IY\", \"OW\", \"OY\", \"UH\", \"UW\", \"UX\"]\n",
    "    \n",
    "    STOP = [\"B\", \"P\", \"D\", \"T\", \"G\", \"K\"]\n",
    "    NASAL = [\"M\", \"N\", \"NG\"]\n",
    "    FRIC = [\"V\", \"DH\", \"F\", \"TH\", \"Z\", \"S\", \"ZH\", \"JH\", \"SH\", \"CH\"]\n",
    "    SON = [\"W\", \"L\", \"R\", \"Y\", \"H\", \"HH\"]\n",
    "    HIGH = [\"IY\", \"IH\", \"OW\", \"AH\", \"UW\", \"UH\", \"ER\"]\n",
    "    LOW = [\"EY\", \"EH\", \"AY\", \"AE\", \"AA\", \"AW\", \"AO\",\"OY\"]\n",
    "    \n",
    "    FRONT = [\"B\", \"P\", \"M\", \"V\", \"DH\", \"F\", \"TH\", \"W\", \"L\", \"IY\", \"IH\", \"EY\", \"EH\"]\n",
    "    MID = [\"D\", \"T\", \"N\", \"Z\", \"S\", \"R\", \"OW\", \"AH\", \"AY\", \"AE\", \"AA\", \"ER\"]\n",
    "    BACK = [\"G\", \"K\", \"NG\", \"ZH\", \"JH\", \"SH\", \"CH\", \"Y\", \"H\", \"HH\", \"UW\", \"UH\", \"AW\", \"AO\", \"OY\"]\n",
    "    \n",
    "    VOICED_LONG = [\"B\", \"D\", \"G\", \"M\", \"N\", \"NG\", \"V\", \"DH\", \"Z\", \"ZH\", \"JH\", \"W\", \"L\", \"R\", \"Y\", \"IY\", \"OW\", \"UW\", \"EY\", \"AY\", \"AW\", \"ER\"]\n",
    "    VOICELESS_SHORT = [\"P\", \"T\", \"K\", \"F\", \"TH\", \"S\", \"SH\", \"CH\", \"H\", \"HH\", \"IH\", \"AH\", \"UH\", \"EH\", \"AE\", \"AA\", \"AO\", \"OY\"]\n",
    "\n",
    "\n",
    "def determine_features(phoneme):\n",
    "    if phoneme == \"#\":\n",
    "        return \"#\"\n",
    "    if phoneme in FEATURES.INTERRUPTED:\n",
    "        art_type = \"interrupted\"\n",
    "    elif phoneme in FEATURES.CONT_CONSONANT:\n",
    "        art_type = \"cont-consonant\"\n",
    "    elif phoneme in FEATURES.VOWEL:\n",
    "        art_type = \"vowel\"\n",
    "    else:\n",
    "        print(\"Error. Phoneme \" + phoneme + \" is not in one of the types.\")\n",
    "        return\n",
    "    \n",
    "    if phoneme in FEATURES.STOP or phoneme in FEATURES.FRIC or phoneme in FEATURES.HIGH:\n",
    "        subtype = \"type1\"\n",
    "    elif phoneme in FEATURES.NASAL or phoneme in FEATURES.SON or phoneme in FEATURES.LOW:\n",
    "        subtype = \"type2\"\n",
    "    else:\n",
    "        print(\"Error. Phoneme \" + phoneme + \" is not in one of the subtypes.\")\n",
    "        return\n",
    "    \n",
    "    if phoneme in FEATURES.FRONT:\n",
    "        place = \"front\"\n",
    "    elif phoneme in FEATURES.MID:\n",
    "        place = \"middle\"\n",
    "    elif phoneme in FEATURES.BACK:\n",
    "        place = \"back\"\n",
    "    else:\n",
    "        print(\"Error. Phoneme \" + phoneme + \" is not in one of the places.\")\n",
    "        return\n",
    "    \n",
    "    if phoneme in FEATURES.VOICED_LONG:\n",
    "        voicing = \"voiced\"\n",
    "    elif phoneme in FEATURES.VOICELESS_SHORT:\n",
    "        voicing = \"voiceless\"\n",
    "    else:\n",
    "        print(\"Error. Phoneme \" + phoneme + \" is not in one of the voicings.\")\n",
    "        return\n",
    "    \n",
    "    return (art_type, subtype, place, voicing)\n",
    "\n",
    "determine_features(\"OW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "outstanding-expert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_vectorize(phoneme):\n",
    "    \"\"\"Creates a vector in the same way as R&M\"\"\"\n",
    "    if phoneme == \"#\":\n",
    "        return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    \n",
    "    (art, subtype, place, voicing) = determine_features(phoneme)\n",
    "    return [int(b) for b in [\n",
    "        art == \"interrupted\",\n",
    "        art == \"cont-consonant\",\n",
    "        art == \"vowel\",\n",
    "        \n",
    "        subtype == \"type1\",\n",
    "        subtype == \"type2\",\n",
    "        \n",
    "        place == \"front\",\n",
    "        place == \"middle\",\n",
    "        place == \"back\",\n",
    "        \n",
    "        voicing == \"voiced\",\n",
    "        voicing == \"voiceless\",\n",
    "        \n",
    "        False\n",
    "    ]]\n",
    "\n",
    "feature_vectorize(\"K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "black-intent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('interrupted', 'vowel', 'interrupted'),\n",
       " ('type1', 'vowel', 'type2'),\n",
       " ('back', 'vowel', 'front'),\n",
       " ('voiceless', 'vowel', 'voiced'),\n",
       " ('interrupted', 'type2', 'interrupted'),\n",
       " ('type1', 'type2', 'type2'),\n",
       " ('back', 'type2', 'front'),\n",
       " ('voiceless', 'type2', 'voiced'),\n",
       " ('interrupted', 'middle', 'interrupted'),\n",
       " ('type1', 'middle', 'type2'),\n",
       " ('back', 'middle', 'front'),\n",
       " ('voiceless', 'middle', 'voiced'),\n",
       " ('interrupted', 'voiceless', 'interrupted'),\n",
       " ('type1', 'voiceless', 'type2'),\n",
       " ('back', 'voiceless', 'front'),\n",
       " ('voiceless', 'voiceless', 'voiced')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wickelfeatures(wickelphone):\n",
    "    \"\"\"Creates the set of 16 Wickelfeatures for a single Wickelphone\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    preceding = determine_features(wickelphone[0])\n",
    "    central = determine_features(wickelphone[1])\n",
    "    following = determine_features(wickelphone[2])\n",
    "    \n",
    "    for central_feature_i in range(4):\n",
    "        central_feature = central[central_feature_i]\n",
    "        for j in range(4):\n",
    "            preceding_feature = \"boundary\" if preceding == \"#\" else preceding[j]\n",
    "            following_feature = \"boundary\" if following == \"#\" else following[j]\n",
    "            features.append((preceding_feature, central_feature, following_feature))\n",
    "    return features\n",
    "\n",
    "wickelfeatures(['K', 'AE', 'M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "communist-moldova",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('back', 'middle', 'boundary'),\n",
       " ('back', 'type1', 'boundary'),\n",
       " ('back', 'voiced', 'boundary'),\n",
       " ('back', 'vowel', 'boundary'),\n",
       " ('boundary', 'back', 'middle'),\n",
       " ('boundary', 'back', 'type1'),\n",
       " ('boundary', 'back', 'voiced'),\n",
       " ('boundary', 'back', 'vowel'),\n",
       " ('boundary', 'interrupted', 'middle'),\n",
       " ('boundary', 'interrupted', 'type1'),\n",
       " ('boundary', 'interrupted', 'voiced'),\n",
       " ('boundary', 'interrupted', 'vowel'),\n",
       " ('boundary', 'type1', 'middle'),\n",
       " ('boundary', 'type1', 'type1'),\n",
       " ('boundary', 'type1', 'voiced'),\n",
       " ('boundary', 'type1', 'vowel'),\n",
       " ('boundary', 'voiced', 'middle'),\n",
       " ('boundary', 'voiced', 'type1'),\n",
       " ('boundary', 'voiced', 'voiced'),\n",
       " ('boundary', 'voiced', 'vowel'),\n",
       " ('interrupted', 'middle', 'boundary'),\n",
       " ('interrupted', 'type1', 'boundary'),\n",
       " ('interrupted', 'voiced', 'boundary'),\n",
       " ('interrupted', 'vowel', 'boundary'),\n",
       " ('type1', 'middle', 'boundary'),\n",
       " ('type1', 'type1', 'boundary'),\n",
       " ('type1', 'voiced', 'boundary'),\n",
       " ('type1', 'vowel', 'boundary'),\n",
       " ('voiced', 'middle', 'boundary'),\n",
       " ('voiced', 'type1', 'boundary'),\n",
       " ('voiced', 'voiced', 'boundary'),\n",
       " ('voiced', 'vowel', 'boundary')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def all_wickelfeatures(word):\n",
    "    features = set() # use a set so elements are unique\n",
    "    all_wickelphones = wickelphones(word)\n",
    "    for phone in all_wickelphones:\n",
    "        for feature in wickelfeatures(phone):\n",
    "            features.add(feature)\n",
    "    return features\n",
    "\n",
    "all_wickelfeatures(find_corpus_item(\"go\", corpus=corpus).stem_phonetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "published-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word in our corpus, determine the wickelfeatures.\n",
    "for item in corpus:\n",
    "    item.wickelfeatures = all_wickelfeatures(item.stem_phonetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-isolation",
   "metadata": {},
   "source": [
    "## Training a simple model\n",
    "At last! We have all the data prepared to create and train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "breathing-event",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('back', 'back', 'back'),\n",
       " ('back', 'back', 'boundary'),\n",
       " ('back', 'back', 'front'),\n",
       " ('back', 'back', 'middle'),\n",
       " ('back', 'cont-consonant', 'back'),\n",
       " ('back', 'cont-consonant', 'boundary'),\n",
       " ('back', 'cont-consonant', 'front'),\n",
       " ('back', 'cont-consonant', 'middle'),\n",
       " ('back', 'front', 'back'),\n",
       " ('back', 'front', 'boundary'),\n",
       " ('back', 'front', 'front'),\n",
       " ('back', 'front', 'middle'),\n",
       " ('back', 'interrupted', 'back'),\n",
       " ('back', 'interrupted', 'boundary'),\n",
       " ('back', 'interrupted', 'front'),\n",
       " ('back', 'interrupted', 'middle'),\n",
       " ('back', 'middle', 'back'),\n",
       " ('back', 'middle', 'boundary'),\n",
       " ('back', 'middle', 'front'),\n",
       " ('back', 'middle', 'middle'),\n",
       " ('back', 'type1', 'back'),\n",
       " ('back', 'type1', 'boundary'),\n",
       " ('back', 'type1', 'front'),\n",
       " ('back', 'type1', 'middle'),\n",
       " ('back', 'type2', 'back'),\n",
       " ('back', 'type2', 'boundary'),\n",
       " ('back', 'type2', 'front'),\n",
       " ('back', 'type2', 'middle'),\n",
       " ('back', 'voiced', 'back'),\n",
       " ('back', 'voiced', 'boundary')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_options = set()\n",
    "for item in corpus:\n",
    "    for feature in item.wickelfeatures:\n",
    "        feature_options.add(feature)\n",
    "        \n",
    "feature_options = sorted(list(feature_options))\n",
    "feature_options[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "historic-edition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem</th>\n",
       "      <th>frequency</th>\n",
       "      <th>morpheme</th>\n",
       "      <th>(back, back, back)</th>\n",
       "      <th>(back, back, boundary)</th>\n",
       "      <th>(back, back, front)</th>\n",
       "      <th>(back, back, middle)</th>\n",
       "      <th>(back, cont-consonant, back)</th>\n",
       "      <th>(back, cont-consonant, boundary)</th>\n",
       "      <th>(back, cont-consonant, front)</th>\n",
       "      <th>...</th>\n",
       "      <th>(vowel, voiced, interrupted)</th>\n",
       "      <th>(vowel, voiced, vowel)</th>\n",
       "      <th>(vowel, voiceless, boundary)</th>\n",
       "      <th>(vowel, voiceless, cont-consonant)</th>\n",
       "      <th>(vowel, voiceless, interrupted)</th>\n",
       "      <th>(vowel, voiceless, vowel)</th>\n",
       "      <th>(vowel, vowel, boundary)</th>\n",
       "      <th>(vowel, vowel, cont-consonant)</th>\n",
       "      <th>(vowel, vowel, interrupted)</th>\n",
       "      <th>(vowel, vowel, vowel)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say</td>\n",
       "      <td>1747</td>\n",
       "      <td>vow change, add /t/</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>produce</td>\n",
       "      <td>28</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>take</td>\n",
       "      <td>426</td>\n",
       "      <td>+ʊ+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recommend</td>\n",
       "      <td>17</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>16</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>drink</td>\n",
       "      <td>1</td>\n",
       "      <td>+ʌ+</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>befall</td>\n",
       "      <td>1</td>\n",
       "      <td>+ɛ+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>tangle</td>\n",
       "      <td>1</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>plumb</td>\n",
       "      <td>1</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>supersede</td>\n",
       "      <td>1</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2070 rows × 466 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           stem  frequency             morpheme  (back, back, back)  \\\n",
       "0           say       1747  vow change, add /t/                   0   \n",
       "1       produce         28                  +ed                   0   \n",
       "2          take        426                  +ʊ+                   0   \n",
       "3     recommend         17                  +ed                   0   \n",
       "4       comment         16                  +ed                   0   \n",
       "...         ...        ...                  ...                 ...   \n",
       "2065      drink          1                  +ʌ+                   0   \n",
       "2066     befall          1                  +ɛ+                   0   \n",
       "2067     tangle          1                  +ed                   0   \n",
       "2068      plumb          1                  +ed                   0   \n",
       "2069  supersede          1                  +ed                   0   \n",
       "\n",
       "      (back, back, boundary)  (back, back, front)  (back, back, middle)  \\\n",
       "0                          0                    0                     0   \n",
       "1                          0                    0                     0   \n",
       "2                          0                    0                     0   \n",
       "3                          0                    0                     0   \n",
       "4                          0                    0                     0   \n",
       "...                      ...                  ...                   ...   \n",
       "2065                       1                    0                     0   \n",
       "2066                       0                    0                     0   \n",
       "2067                       0                    0                     1   \n",
       "2068                       0                    0                     0   \n",
       "2069                       0                    0                     0   \n",
       "\n",
       "      (back, cont-consonant, back)  (back, cont-consonant, boundary)  \\\n",
       "0                                0                                 0   \n",
       "1                                0                                 1   \n",
       "2                                0                                 0   \n",
       "3                                0                                 0   \n",
       "4                                0                                 0   \n",
       "...                            ...                               ...   \n",
       "2065                             0                                 0   \n",
       "2066                             0                                 1   \n",
       "2067                             0                                 0   \n",
       "2068                             0                                 0   \n",
       "2069                             0                                 0   \n",
       "\n",
       "      (back, cont-consonant, front)  ...  (vowel, voiced, interrupted)  \\\n",
       "0                                 0  ...                             0   \n",
       "1                                 0  ...                             0   \n",
       "2                                 0  ...                             0   \n",
       "3                                 0  ...                             1   \n",
       "4                                 0  ...                             1   \n",
       "...                             ...  ...                           ...   \n",
       "2065                              0  ...                             1   \n",
       "2066                              0  ...                             0   \n",
       "2067                              0  ...                             1   \n",
       "2068                              0  ...                             0   \n",
       "2069                              0  ...                             0   \n",
       "\n",
       "      (vowel, voiced, vowel)  (vowel, voiceless, boundary)  \\\n",
       "0                          0                             0   \n",
       "1                          1                             1   \n",
       "2                          0                             1   \n",
       "3                          1                             0   \n",
       "4                          1                             0   \n",
       "...                      ...                           ...   \n",
       "2065                       0                             0   \n",
       "2066                       0                             0   \n",
       "2067                       0                             0   \n",
       "2068                       0                             0   \n",
       "2069                       0                             0   \n",
       "\n",
       "      (vowel, voiceless, cont-consonant)  (vowel, voiceless, interrupted)  \\\n",
       "0                                      0                                0   \n",
       "1                                      0                                0   \n",
       "2                                      0                                0   \n",
       "3                                      0                                0   \n",
       "4                                      0                                0   \n",
       "...                                  ...                              ...   \n",
       "2065                                   0                                0   \n",
       "2066                                   0                                0   \n",
       "2067                                   0                                0   \n",
       "2068                                   0                                0   \n",
       "2069                                   0                                0   \n",
       "\n",
       "      (vowel, voiceless, vowel)  (vowel, vowel, boundary)  \\\n",
       "0                             0                         0   \n",
       "1                             0                         0   \n",
       "2                             0                         0   \n",
       "3                             1                         0   \n",
       "4                             0                         0   \n",
       "...                         ...                       ...   \n",
       "2065                          0                         0   \n",
       "2066                          1                         0   \n",
       "2067                          0                         0   \n",
       "2068                          0                         0   \n",
       "2069                          1                         0   \n",
       "\n",
       "      (vowel, vowel, cont-consonant)  (vowel, vowel, interrupted)  \\\n",
       "0                                  0                            0   \n",
       "1                                  0                            0   \n",
       "2                                  0                            0   \n",
       "3                                  0                            0   \n",
       "4                                  0                            0   \n",
       "...                              ...                          ...   \n",
       "2065                               0                            0   \n",
       "2066                               0                            0   \n",
       "2067                               0                            0   \n",
       "2068                               0                            0   \n",
       "2069                               0                            0   \n",
       "\n",
       "      (vowel, vowel, vowel)  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "...                     ...  \n",
       "2065                      0  \n",
       "2066                      0  \n",
       "2067                      0  \n",
       "2068                      0  \n",
       "2069                      0  \n",
       "\n",
       "[2070 rows x 466 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for item in corpus:\n",
    "    row = []\n",
    "    row.append(item.stem)\n",
    "    row.append(item.frequency)\n",
    "    row.append(item.morpheme)\n",
    "    for feature in feature_options:\n",
    "        if feature in item.wickelfeatures:\n",
    "            row.append(1)\n",
    "        else:\n",
    "            row.append(0)\n",
    "    data.append(row)\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data.columns = [\"stem\", \"frequency\", \"morpheme\"] + feature_options\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alone-reform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem</th>\n",
       "      <th>frequency</th>\n",
       "      <th>morpheme</th>\n",
       "      <th>(back, back, back)</th>\n",
       "      <th>(back, back, boundary)</th>\n",
       "      <th>(back, back, front)</th>\n",
       "      <th>(back, back, middle)</th>\n",
       "      <th>(back, cont-consonant, back)</th>\n",
       "      <th>(back, cont-consonant, boundary)</th>\n",
       "      <th>(back, cont-consonant, front)</th>\n",
       "      <th>...</th>\n",
       "      <th>(vowel, voiced, interrupted)</th>\n",
       "      <th>(vowel, voiced, vowel)</th>\n",
       "      <th>(vowel, voiceless, boundary)</th>\n",
       "      <th>(vowel, voiceless, cont-consonant)</th>\n",
       "      <th>(vowel, voiceless, interrupted)</th>\n",
       "      <th>(vowel, voiceless, vowel)</th>\n",
       "      <th>(vowel, vowel, boundary)</th>\n",
       "      <th>(vowel, vowel, cont-consonant)</th>\n",
       "      <th>(vowel, vowel, interrupted)</th>\n",
       "      <th>(vowel, vowel, vowel)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say</td>\n",
       "      <td>1747</td>\n",
       "      <td>vow change, add /t/</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say</td>\n",
       "      <td>1747</td>\n",
       "      <td>vow change, add /t/</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say</td>\n",
       "      <td>1747</td>\n",
       "      <td>vow change, add /t/</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say</td>\n",
       "      <td>1747</td>\n",
       "      <td>vow change, add /t/</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say</td>\n",
       "      <td>1747</td>\n",
       "      <td>vow change, add /t/</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>drink</td>\n",
       "      <td>1</td>\n",
       "      <td>+ʌ+</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>befall</td>\n",
       "      <td>1</td>\n",
       "      <td>+ɛ+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>tangle</td>\n",
       "      <td>1</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>plumb</td>\n",
       "      <td>1</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>supersede</td>\n",
       "      <td>1</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25828 rows × 466 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           stem  frequency             morpheme  (back, back, back)  \\\n",
       "0           say       1747  vow change, add /t/                   0   \n",
       "0           say       1747  vow change, add /t/                   0   \n",
       "0           say       1747  vow change, add /t/                   0   \n",
       "0           say       1747  vow change, add /t/                   0   \n",
       "0           say       1747  vow change, add /t/                   0   \n",
       "...         ...        ...                  ...                 ...   \n",
       "2065      drink          1                  +ʌ+                   0   \n",
       "2066     befall          1                  +ɛ+                   0   \n",
       "2067     tangle          1                  +ed                   0   \n",
       "2068      plumb          1                  +ed                   0   \n",
       "2069  supersede          1                  +ed                   0   \n",
       "\n",
       "      (back, back, boundary)  (back, back, front)  (back, back, middle)  \\\n",
       "0                          0                    0                     0   \n",
       "0                          0                    0                     0   \n",
       "0                          0                    0                     0   \n",
       "0                          0                    0                     0   \n",
       "0                          0                    0                     0   \n",
       "...                      ...                  ...                   ...   \n",
       "2065                       1                    0                     0   \n",
       "2066                       0                    0                     0   \n",
       "2067                       0                    0                     1   \n",
       "2068                       0                    0                     0   \n",
       "2069                       0                    0                     0   \n",
       "\n",
       "      (back, cont-consonant, back)  (back, cont-consonant, boundary)  \\\n",
       "0                                0                                 0   \n",
       "0                                0                                 0   \n",
       "0                                0                                 0   \n",
       "0                                0                                 0   \n",
       "0                                0                                 0   \n",
       "...                            ...                               ...   \n",
       "2065                             0                                 0   \n",
       "2066                             0                                 1   \n",
       "2067                             0                                 0   \n",
       "2068                             0                                 0   \n",
       "2069                             0                                 0   \n",
       "\n",
       "      (back, cont-consonant, front)  ...  (vowel, voiced, interrupted)  \\\n",
       "0                                 0  ...                             0   \n",
       "0                                 0  ...                             0   \n",
       "0                                 0  ...                             0   \n",
       "0                                 0  ...                             0   \n",
       "0                                 0  ...                             0   \n",
       "...                             ...  ...                           ...   \n",
       "2065                              0  ...                             1   \n",
       "2066                              0  ...                             0   \n",
       "2067                              0  ...                             1   \n",
       "2068                              0  ...                             0   \n",
       "2069                              0  ...                             0   \n",
       "\n",
       "      (vowel, voiced, vowel)  (vowel, voiceless, boundary)  \\\n",
       "0                          0                             0   \n",
       "0                          0                             0   \n",
       "0                          0                             0   \n",
       "0                          0                             0   \n",
       "0                          0                             0   \n",
       "...                      ...                           ...   \n",
       "2065                       0                             0   \n",
       "2066                       0                             0   \n",
       "2067                       0                             0   \n",
       "2068                       0                             0   \n",
       "2069                       0                             0   \n",
       "\n",
       "      (vowel, voiceless, cont-consonant)  (vowel, voiceless, interrupted)  \\\n",
       "0                                      0                                0   \n",
       "0                                      0                                0   \n",
       "0                                      0                                0   \n",
       "0                                      0                                0   \n",
       "0                                      0                                0   \n",
       "...                                  ...                              ...   \n",
       "2065                                   0                                0   \n",
       "2066                                   0                                0   \n",
       "2067                                   0                                0   \n",
       "2068                                   0                                0   \n",
       "2069                                   0                                0   \n",
       "\n",
       "      (vowel, voiceless, vowel)  (vowel, vowel, boundary)  \\\n",
       "0                             0                         0   \n",
       "0                             0                         0   \n",
       "0                             0                         0   \n",
       "0                             0                         0   \n",
       "0                             0                         0   \n",
       "...                         ...                       ...   \n",
       "2065                          0                         0   \n",
       "2066                          1                         0   \n",
       "2067                          0                         0   \n",
       "2068                          0                         0   \n",
       "2069                          1                         0   \n",
       "\n",
       "      (vowel, vowel, cont-consonant)  (vowel, vowel, interrupted)  \\\n",
       "0                                  0                            0   \n",
       "0                                  0                            0   \n",
       "0                                  0                            0   \n",
       "0                                  0                            0   \n",
       "0                                  0                            0   \n",
       "...                              ...                          ...   \n",
       "2065                               0                            0   \n",
       "2066                               0                            0   \n",
       "2067                               0                            0   \n",
       "2068                               0                            0   \n",
       "2069                               0                            0   \n",
       "\n",
       "      (vowel, vowel, vowel)  \n",
       "0                         0  \n",
       "0                         0  \n",
       "0                         0  \n",
       "0                         0  \n",
       "0                         0  \n",
       "...                     ...  \n",
       "2065                      0  \n",
       "2066                      0  \n",
       "2067                      0  \n",
       "2068                      0  \n",
       "2069                      0  \n",
       "\n",
       "[25828 rows x 466 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_frequency = data.loc[data.index.repeat(data.frequency)]\n",
    "data_with_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "attractive-episode",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem</th>\n",
       "      <th>frequency</th>\n",
       "      <th>morpheme</th>\n",
       "      <th>(back, back, back)</th>\n",
       "      <th>(back, back, boundary)</th>\n",
       "      <th>(back, back, front)</th>\n",
       "      <th>(back, back, middle)</th>\n",
       "      <th>(back, cont-consonant, back)</th>\n",
       "      <th>(back, cont-consonant, boundary)</th>\n",
       "      <th>(back, cont-consonant, front)</th>\n",
       "      <th>...</th>\n",
       "      <th>(vowel, voiced, interrupted)</th>\n",
       "      <th>(vowel, voiced, vowel)</th>\n",
       "      <th>(vowel, voiceless, boundary)</th>\n",
       "      <th>(vowel, voiceless, cont-consonant)</th>\n",
       "      <th>(vowel, voiceless, interrupted)</th>\n",
       "      <th>(vowel, voiceless, vowel)</th>\n",
       "      <th>(vowel, vowel, boundary)</th>\n",
       "      <th>(vowel, vowel, cont-consonant)</th>\n",
       "      <th>(vowel, vowel, interrupted)</th>\n",
       "      <th>(vowel, vowel, vowel)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>walk</td>\n",
       "      <td>143</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>stand</td>\n",
       "      <td>198</td>\n",
       "      <td>suppletion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>want</td>\n",
       "      <td>204</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>fail</td>\n",
       "      <td>52</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>want</td>\n",
       "      <td>204</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>turn</td>\n",
       "      <td>253</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>get</td>\n",
       "      <td>338</td>\n",
       "      <td>+ɑ+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>fall</td>\n",
       "      <td>87</td>\n",
       "      <td>+ɛ+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>carry</td>\n",
       "      <td>60</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>dislike</td>\n",
       "      <td>11</td>\n",
       "      <td>+ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5166 rows × 466 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stem  frequency    morpheme  (back, back, back)  \\\n",
       "267      walk        143         +ed                   0   \n",
       "32      stand        198  suppletion                   0   \n",
       "16       want        204         +ed                   0   \n",
       "131      fail         52         +ed                   0   \n",
       "16       want        204         +ed                   0   \n",
       "...       ...        ...         ...                 ...   \n",
       "153      turn        253         +ed                   0   \n",
       "31        get        338         +ɑ+                   0   \n",
       "307      fall         87         +ɛ+                   0   \n",
       "242     carry         60         +ed                   0   \n",
       "1079  dislike         11         +ed                   0   \n",
       "\n",
       "      (back, back, boundary)  (back, back, front)  (back, back, middle)  \\\n",
       "267                        1                    0                     0   \n",
       "32                         0                    0                     0   \n",
       "16                         0                    0                     0   \n",
       "131                        0                    0                     0   \n",
       "16                         0                    0                     0   \n",
       "...                      ...                  ...                   ...   \n",
       "153                        0                    0                     0   \n",
       "31                         0                    0                     0   \n",
       "307                        0                    0                     0   \n",
       "242                        0                    0                     0   \n",
       "1079                       0                    0                     0   \n",
       "\n",
       "      (back, cont-consonant, back)  (back, cont-consonant, boundary)  \\\n",
       "267                              0                                 0   \n",
       "32                               0                                 0   \n",
       "16                               0                                 0   \n",
       "131                              0                                 0   \n",
       "16                               0                                 0   \n",
       "...                            ...                               ...   \n",
       "153                              0                                 0   \n",
       "31                               0                                 0   \n",
       "307                              0                                 1   \n",
       "242                              0                                 0   \n",
       "1079                             0                                 0   \n",
       "\n",
       "      (back, cont-consonant, front)  ...  (vowel, voiced, interrupted)  \\\n",
       "267                               0  ...                             0   \n",
       "32                                0  ...                             1   \n",
       "16                                0  ...                             1   \n",
       "131                               0  ...                             0   \n",
       "16                                0  ...                             1   \n",
       "...                             ...  ...                           ...   \n",
       "153                               0  ...                             0   \n",
       "31                                0  ...                             0   \n",
       "307                               0  ...                             0   \n",
       "242                               0  ...                             0   \n",
       "1079                              0  ...                             0   \n",
       "\n",
       "      (vowel, voiced, vowel)  (vowel, voiceless, boundary)  \\\n",
       "267                        0                             1   \n",
       "32                         0                             0   \n",
       "16                         0                             0   \n",
       "131                        0                             0   \n",
       "16                         0                             0   \n",
       "...                      ...                           ...   \n",
       "153                        0                             0   \n",
       "31                         0                             1   \n",
       "307                        0                             0   \n",
       "242                        1                             0   \n",
       "1079                       0                             1   \n",
       "\n",
       "      (vowel, voiceless, cont-consonant)  (vowel, voiceless, interrupted)  \\\n",
       "267                                    0                                0   \n",
       "32                                     0                                0   \n",
       "16                                     0                                0   \n",
       "131                                    0                                0   \n",
       "16                                     0                                0   \n",
       "...                                  ...                              ...   \n",
       "153                                    0                                0   \n",
       "31                                     0                                0   \n",
       "307                                    0                                0   \n",
       "242                                    0                                0   \n",
       "1079                                   1                                0   \n",
       "\n",
       "      (vowel, voiceless, vowel)  (vowel, vowel, boundary)  \\\n",
       "267                           0                         0   \n",
       "32                            0                         0   \n",
       "16                            0                         0   \n",
       "131                           0                         0   \n",
       "16                            0                         0   \n",
       "...                         ...                       ...   \n",
       "153                           0                         0   \n",
       "31                            0                         0   \n",
       "307                           0                         0   \n",
       "242                           0                         0   \n",
       "1079                          0                         0   \n",
       "\n",
       "      (vowel, vowel, cont-consonant)  (vowel, vowel, interrupted)  \\\n",
       "267                                0                            0   \n",
       "32                                 0                            0   \n",
       "16                                 0                            0   \n",
       "131                                0                            0   \n",
       "16                                 0                            0   \n",
       "...                              ...                          ...   \n",
       "153                                0                            0   \n",
       "31                                 0                            0   \n",
       "307                                0                            0   \n",
       "242                                0                            0   \n",
       "1079                               0                            0   \n",
       "\n",
       "      (vowel, vowel, vowel)  \n",
       "267                       0  \n",
       "32                        0  \n",
       "16                        0  \n",
       "131                       0  \n",
       "16                        0  \n",
       "...                     ...  \n",
       "153                       0  \n",
       "31                        0  \n",
       "307                       0  \n",
       "242                       0  \n",
       "1079                      0  \n",
       "\n",
       "[5166 rows x 466 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split testing data for evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data_with_frequency, test_size=0.2)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-kentucky",
   "metadata": {},
   "source": [
    "Let's use this data to train a simple neural network classifier to make sure everything looks good. Later on we'll tune the network for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "remarkable-packaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.8718904268705837\n",
      "Test:  0.8627564847077043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(1, 10), random_state=1, max_iter=1000)\n",
    "clf.fit(train[feature_options], train[\"morpheme\"])\n",
    "print(\"Train: \", clf.score(train[feature_options], train[\"morpheme\"]))\n",
    "print(\"Test: \", clf.score(test[feature_options], test[\"morpheme\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "trying-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+ed']\n",
      "['+æ+']\n",
      "[['R', 'EH1', 'D'], ['R', 'IY1', 'D']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which one? 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+ed']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(word, model, pronounciation=None):\n",
    "    if not pronounciation:\n",
    "        pronounciations = pronounce(word)\n",
    "        if len(pronounciations) == 1:\n",
    "            pronounciation = pronounciations[0]\n",
    "        else:\n",
    "            print(pronounciations)\n",
    "            choice = input(\"Which one?\")\n",
    "            pronounciation = pronounciations[int(choice)]\n",
    "        \n",
    "    x = []\n",
    "    for feature in feature_options:\n",
    "        if feature in all_wickelfeatures([re.sub(r'\\d+', '', alt) for alt in pronounciation]):\n",
    "            x.append(1)\n",
    "        else:\n",
    "            x.append(0)\n",
    "    return model.predict(np.array([x]))\n",
    "    \n",
    "# Wug test\n",
    "print(predict(\"wug\", clf, ['W', 'AH1', 'G']))\n",
    "print(predict(\"sing\", clf))\n",
    "print(predict(\"read\", clf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-newcastle",
   "metadata": {},
   "source": [
    "### Tuning Model\n",
    "We can see that we have created a neural network that performs with relatively high accuracy on the training and test data. It also preserves knowledge about irregular forms, as demonstrated above. Now, we will attempt to improve our model with 10-fold cross validation. This will take very long to run, so feel free to skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "crude-support",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trying x1=3, x2=9'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(9, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def cross_validate(eval_function, k):\n",
    "    kf = KFold(n_splits=10)\n",
    "    \n",
    "    all_accuracies = []\n",
    "    \n",
    "    for train_folds, test_fold in kf.split(train):\n",
    "        (accuracy, _, _)  = eval_function(k, \n",
    "                                 train[feature_options].iloc[train_folds], \n",
    "                                 train['morpheme'].iloc[train_folds].values.ravel(), \n",
    "                                 train[feature_options].iloc[test_fold], \n",
    "                                 train['morpheme'].iloc[test_fold].values.ravel())\n",
    "        all_accuracies.append(accuracy)\n",
    "    \n",
    "    return 1 - (sum(all_accuracies) / len(all_accuracies))\n",
    "\n",
    "def tune_multiple_hyperparameters(eval_function, x1_range, x2_range):\n",
    "    all_errors = [] # 2d array\n",
    "    min_error = 1\n",
    "    min_error_indices = (-1, -1)\n",
    "    \n",
    "    for x1 in x1_range:\n",
    "        x1_vals = []\n",
    "        for x2 in x2_range:\n",
    "            clear_output(wait=True)\n",
    "            display(f\"Trying x1={x1}, x2={x2}\")\n",
    "            result = cross_validate(eval_function, (x1, x2))\n",
    "            \n",
    "            # Keep track of the lowest error\n",
    "            if result < min_error:\n",
    "                min_error = result\n",
    "                min_error_indices = (x2, x1)\n",
    "                \n",
    "            x1_vals.append(result)\n",
    "        all_errors.append(x1_vals)\n",
    "    \n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.pcolormesh(x2_range, x1_range, all_errors)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    return min_error_indices\n",
    "\n",
    "def eval_nn_sigmoid(hidden_layer_sizes, X_train, y_train, X_val, y_val):\n",
    "    start_train = time.time()\n",
    "    nn = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation='logistic', max_iter=250)\n",
    "    nn.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    start_predict = time.time()\n",
    "    y_pred = nn.predict(X_val)\n",
    "    end_predict = time.time()\n",
    "    return (metrics.accuracy_score(y_val, y_pred), end_train - start_train, end_predict - start_predict)\n",
    "\n",
    "tune_multiple_hyperparameters(eval_nn_sigmoid, range(1, 4), range(1, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-classic",
   "metadata": {},
   "source": [
    "The above cell takes very long to run, but suggests (3,9) as the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "christian-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.9945794211596167\n",
      "Test:  0.9905149051490515\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(3, 9), random_state=1, max_iter=1000)\n",
    "clf.fit(train[feature_options], train[\"morpheme\"])\n",
    "print(\"Train: \", clf.score(train[feature_options], train[\"morpheme\"]))\n",
    "print(\"Test: \", clf.score(test[feature_options], test[\"morpheme\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cutting-infrared",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+ed']\n",
      "['+u+']\n",
      "[['R', 'EH1', 'D'], ['R', 'IY1', 'D']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which one? 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+ɛ+']\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"wug\", clf, ['W', 'AH1', 'G']))\n",
    "print(predict(\"zing\", clf, ['Z', 'IH1', 'NG']))\n",
    "print(predict(\"read\", clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-leather",
   "metadata": {},
   "source": [
    "### Results\n",
    "With these hyperparameters, our model performs extremely well on both the training and testing data. Furthermore, it makes the same predictions in a wug test as a human. \n",
    "\n",
    "The R&M paper does not give an overall number for accuracy of their entire model. However, it lists that overall, 91% of wickelfeatures were correctly generated for novel (test) data. Thus, our model must be better, since 99% of test data is predicted exactly correct (all wickelfeatures correct). Granted, this is comparing apples to oranges, since our model has the additional power of only predicting morphemes and ignoring phonology.\n",
    "\n",
    "In fact, this seems to support our hypothesis, that by distinguishing between morphology and phonology, and by building models that operate over only one of these areas, we can create much more accurate models. Consequently, this experiment provides evidence that morphology and phonology can be more effectively modeled as separate processes. \n",
    "\n",
    "In addition, our hypotheses about what factors would improve the model were correct. The optimal shape for the model was a deep network, indicating that our data is not linearly separable. Naturally, we benefit from modern theories and technologies, but it is fascinating to see how much we can build on R&M's model and improve it.\n",
    "\n",
    "## Nearest Neighbors\n",
    "Last, we will repeat this with a nearest neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "lyric-township",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trying k=9'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def tune_hyperparameter(eval_function, k_range):\n",
    "    all_errors = []\n",
    "    min_error = 1\n",
    "    min_error_index = -1\n",
    "    \n",
    "    for k in k_range:\n",
    "        clear_output(wait=True)\n",
    "        display(f\"Trying k={k}\")\n",
    "        result = cross_validate(eval_function, k)\n",
    "        \n",
    "        if result < min_error:\n",
    "            min_error = result\n",
    "            min_error_index = k\n",
    "            \n",
    "        all_errors.append(result)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    ax1 = plt.subplot()\n",
    "    ax1.set_xticks(k_range)\n",
    "    ax1.plot(k_range, all_errors)\n",
    "    \n",
    "    return min_error_index\n",
    "\n",
    "def eval_knn_classifier(k, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Returns a tuple containing (accuracy, training time, testing time)\"\"\"\n",
    "    start_train = time.time()\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    start_predict = time.time()\n",
    "    y_pred = knn.predict(X_val)\n",
    "    end_predict = time.time()\n",
    "    return (metrics.accuracy_score(y_val, y_pred), end_train - start_train, end_predict - start_predict)\n",
    "\n",
    "tune_hyperparameter(eval_knn_classifier, range(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-planning",
   "metadata": {},
   "source": [
    "We can see that the performance actually worsens with greater k. Let's build the model with `k=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "brave-seminar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.9945794211596167\n",
      "Test:  0.9905149051490515\n",
      "[<MORPHEME.ED: '+ed'>]\n",
      "['+u+']\n",
      "[['R', 'EH1', 'D'], ['R', 'IY1', 'D']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which one? 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<MORPHEME.EH: '+ɛ+'>]\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(train[feature_options], train[\"morpheme\"])\n",
    "\n",
    "print(\"Train: \", clf.score(train[feature_options], train[\"morpheme\"]))\n",
    "print(\"Test: \", clf.score(test[feature_options], test[\"morpheme\"]))\n",
    "\n",
    "print(predict(\"wug\", knn, ['W', 'AH1', 'G']))\n",
    "print(predict(\"zing\", clf, ['Z', 'IH1', 'NG']))\n",
    "print(predict(\"read\", knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-divorce",
   "metadata": {},
   "source": [
    "The nearest neighbors classifier also does extremely well on the data. One of the goals of a combined connectionist/generative model is the ability to gain information from looking at models that learn patterns. In this case, we see evidence for the theory that the primary way in which we determine which morpheme to use is by comparing a novel word to other words in terms of phonetic similarity. We can see that simply by using the closest known word, we can get very accurate results.\n",
    "\n",
    "# Conclusion\n",
    "With easy-to-use libraries and simple data processing, we have been able to replicate and improve on the Rumelhart and McClelland model for learning English past tense with a model that combines connectionist theory and morphological information. This is a promising example of how embedding generativist information can vastly improve our ability to build effective models, and to close to gap of the two schools of thought.\n",
    "\n",
    "Furthermore, it is striking how much our ability to build, train, and test models has come from 1985. As our technology continues to advance, along with our understanding of psycholinguistics, we can build models that come closer and closer to the human brain, and help us unravel the mysteries of language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-gibraltar",
   "metadata": {},
   "source": [
    "## References\n",
    "Plunkett, K., & Juola, P. (1999). A connectionist model of English past tense and plural morphology. Cognitive Science, 23(4), 463-490.\n",
    "\n",
    "Rueckl, J. G., & Raveh, M. (1999). The influence of morphological regularities on the dynamics of a connectionist network. Brain and Language, 68(1-2), 110-117.\n",
    "\n",
    "Rumelhart, D. E., & McClelland, J. L. (1985). On learning the past tenses of English verbs. CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-photography",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
